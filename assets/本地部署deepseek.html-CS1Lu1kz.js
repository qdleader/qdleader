import{_ as a,c as s,a as p,o as n}from"./app-KfnfuIf0.js";const l={};function r(t,e){return n(),s("div",null,[...e[0]||(e[0]=[p(`<h1 id="本地部署-deepseek" tabindex="-1"><a class="header-anchor" href="#本地部署-deepseek"><span>本地部署 deepseek</span></a></h1><h2 id="一、为什么要部署本地-deepseek" tabindex="-1"><a class="header-anchor" href="#一、为什么要部署本地-deepseek"><span>一、为什么要部署本地 DeepSeek？</span></a></h2><p>相信大家在使用 DeepSeek 时都会遇到这样的问题：</p><p>这是由于 DeepSeek 大火之后访问量比较大，再加上漂亮国大规模、持续的恶意攻击，导致 DeepSeek 的服务器很不稳定。所以，这个此时在本地部署一个 DeepSeek 大模型就非常有必要了。</p><p>再者说，有些数据比较敏感，咱也不想随便传到网上去，本地大模型可以根据你的需求进行定制，想怎么用就怎么用，灵活性超强！</p><h2 id="二、怎么部署本地大模型" tabindex="-1"><a class="header-anchor" href="#二、怎么部署本地大模型"><span>二、怎么部署本地大模型？</span></a></h2><p>在本地部署 DeepSeek 只需要以下三步：</p><p>安装 Ollama。 部署 DeepSeek。 使用 DeepSeek：这里我们使用 ChatBox 客户端操作 DeepSeek（此步骤非必须）。 Ollama、DeepSeek 和 ChatBox 之间的关系如下：</p><p>Ollama 是“大管家”，负责把 DeepSeek 安装到你的电脑上。 DeepSeek 是“超级大脑”，住在 Ollama 搭建好的环境里，帮你做各种事情。 ChatBox 是“聊天工具”，让你更方便地和 DeepSeek 交流。 安装 Ollama Ollama 是一个开源的大型语言模型服务工具。它的主要作用是帮助用户快速在本地运行大模型，简化了在 Docker 容器内部署和管理大语言模型（LLM）的过程。</p><p>PS：Ollama 就是大模型届的“Docker”。</p><p>Ollama 优点如下：</p><p>易于使用：即使是没有经验的用户也能轻松上手，无需开发即可直接与模型进行交互。 轻量级：代码简洁，运行时占用资源少，能够在本地高效运行，不需要大量的计算资源。 可扩展：支持多种模型架构，并易于添加新模型或更新现有模型，还支持热加载模型文件，无需重新启动即可切换不同的模型，具有较高的灵活性。 预构建模型库：包含一系列预先训练好的大型语言模型，可用于各种任务，如文本生成、翻译、问答等，方便在本地运行大型语言模型。</p><p>下载并安装 Ollama 下载地址：https://ollama.com/</p><p>mac 下载完后，选择一个模型，然后终端中打开 输入 你选择的模型命令 即可</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">ollama run deepseek-r1:1.5b</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>此时你就可以在命令行中使用 DeepSeek 了。</p><p>怎么得到一个可视化页面的呢</p><p>在 Docker 应用上添加一个 Open-WebUI 组件，让 DeepSeek-R1 可以通过浏览器界面交互，并赋予它联系上下文的能力。</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 --add-host<span class="token operator">=</span>host.docker.internal:host-gateway <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token parameter variable">--name</span> open-webui <span class="token parameter variable">--restart</span> always ghcr.io/open-webui/open-webui:main</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div>`,19)])])}const i=a(l,[["render",r]]),o=JSON.parse('{"path":"/ai/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2deepseek.html","title":"本地部署 deepseek","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768183010000,"contributors":[{"name":"qdleader","username":"qdleader","email":"yk4545945@163.com","commits":1,"url":"https://github.com/qdleader"}],"changelog":[{"hash":"c3ab6d103d428cdd6a361e045add7154fc9253f1","time":1768183010000,"email":"yk4545945@163.com","author":"qdleader","message":"docs(mcp): 新增Mastergo MCP配置文档，包含token获取方法"}]},"filePathRelative":"ai/本地部署deepseek.md"}');export{i as comp,o as data};
